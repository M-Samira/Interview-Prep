{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "777ce89c",
   "metadata": {},
   "source": [
    "This Notebook includes soem of the most popular Pytorch interview topics with examples and comments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "848bf6af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8., grad_fn=<AddBackward0>)\n",
      "tensor([2., 2.])\n"
     ]
    }
   ],
   "source": [
    "#gradient wrt a tensor\n",
    "tensor = torch.tensor([3.0, 1.0], requires_grad=True)\n",
    "\n",
    "# Multiply with another Tensor\n",
    "result = sum(tensor * 2)\n",
    "print(result)\n",
    "# Obtain the gradient\n",
    "result.backward()\n",
    "print(tensor.grad) #grad is only used for scaler outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ef184d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n",
      "tensor(6.)\n"
     ]
    }
   ],
   "source": [
    "#Taking gradient\n",
    "import torch\n",
    "\n",
    "# Define tensors\n",
    "x = torch.tensor(3., requires_grad=True)\n",
    "y = torch.tensor(4., requires_grad=True)\n",
    "z = 2*x*y + 3\n",
    "\n",
    "# Visualize the graph\n",
    "z.backward()\n",
    "#gradient of z wrt x and y\n",
    "print(x.grad)\n",
    "print(y.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c32ed52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to have reprducible results?\n",
    "# fix seed : torch.manual_seed(0) or torch.cuda.manual_seed(9)\n",
    "# use deterministic algorithms : torch.use_deterministic_algorithms() --> example?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5a61edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "# reshape tensor dimension:\n",
    "x = torch.ones(3,4)\n",
    "x.view(4,1,3)\n",
    "print(x)\n",
    "# .reshape() and .resize() also change dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45fda2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## matrix multiplication \n",
    "# torch.matmul and torch.bmm for batch computations \n",
    "# also A @ B performs matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5bb2b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## freeze some parameters of \"model\" (e.g., freeze layer with name fc1)\n",
    "# for name, param in model.named_parameters():\n",
    "#     if param.requires_grad and 'fc1' in name:\n",
    "#         param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b823501e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## change learning rate: use learnign rate scheduler --> step decay : StepLR\n",
    "# import torch.optim.lr_scheduler as lr_scheduler\n",
    "# scheduler = lr_scheduler.StepLR(optimizer, step_size = 5, gamma = 0.1) every 5 epochs decrease learning rate by 10%\n",
    "# at the end of each epoch in training loop add scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa0464ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "## set up pytorch to run on GPU:\n",
    "# device = torch.device('cuda:0'if torch.cuda.is_available() esle 'cpu')\n",
    "# model.to(device)\n",
    "## run model on multiple GPUs:\n",
    "# if torch.cuda.device_count() > 1: model = nn.DataParallel(model)\n",
    "## Do something in a specific GPU:\n",
    "# with torch.cuda.device(1) :  xxx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "74b90a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to initialzie parametrs in NN \n",
    "## if using built in layers there is default initialization but we may need to change or build custom layers:\n",
    "#     for p in model.parameters():\n",
    "#         if p.dim() > 1:   #if p is a weight tensor\n",
    "#             nn.init.xavier_uniform_(p)\n",
    "#         else:   #if p is a bias\n",
    "#             nn.init.constant_(p, 0) #initialize to zero\n",
    "#     return model\n",
    "# other weight initializations : kaiming_normal_ for linear layers and orthogonal_ for conv layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a0dfdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a simple NN and train and evaluate / 28*28 images\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init()\n",
    "        self.fc1 = nn.Linear(28*28,32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) #x:batch_size*10*1\n",
    "        return F.log_softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba6f709",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "import torch.optim as optim\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_function = nn.MSELoss()\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    for batch, X, y in trainset:\n",
    "        y_pred = model(X.view(-1,28*28))\n",
    "        loss = loss_function(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimzer.step\n",
    "print(loss)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46d9b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate on test set (batched!)\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for x, y in testset:\n",
    "        y_pred = model(x.view(-1,28*28))\n",
    "        for idx, i in enumerate(y_pred):\n",
    "            if torch.argmax(i) == y[idx]:\n",
    "                correct += 1\n",
    "            total+=1\n",
    "    accuracy = correct/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce3d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom layer\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_features, output_features, custom_param):\n",
    "        super().__init__()\n",
    "        self.weights = nn.Parameter(torch.Tensor(output_features, input_features))\n",
    "        self.bias = None\n",
    "        self.initialize()\n",
    "        self.custom_param = custom_param\n",
    "    def forward(self, x):\n",
    "        x = F.linear(x, self.weights, self.bias)\n",
    "    def initialzie(self):\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e76acc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Gardient Clipping: torch.nn.utils.clip_grad_norm_() to prevent gradient exploding especially in RNNs LSTM GRUs\n",
    "## if L2 norm of gradient goes above a certain limit --> clip the gradient\n",
    "### da da da \n",
    "loss.backward()\n",
    "\n",
    "# Clip gradients with norm-based methods\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25) #scales down gradient so norm is below 0.25\n",
    "torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=0.1) #clip the grad so each individual element's. absolute value is <= 0.1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fe678",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Detect and Decrease Overfitting: \n",
    "#Detect: visulaize learning curve TensorboardX; crossvalidate after each epoch\n",
    "#Decrease:\n",
    "#1 Regularization : weight regularization in loss function, dropout (nn.Dropout()), data augmentation\n",
    "# batch normalization (nn.BatchNorm1d(size)), layer normalization (nn.LayerNorm([features]))\n",
    "#2 Hyperparam tunning \n",
    "#3 Early stopping: calculate validation error after each epoch! if not improving after a predefined number of epochs\n",
    "# stop and save the best model along the way:\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# create datasets, assume we loaded train_data and validate_data from torchvisison \n",
    "train_set = torch.utils.DataLoader(train_data, batch_size = 64, shuffle = True)\n",
    "val_set = torch.utils.DataLoader(validate_data, batch_size = 64, shuffle = False)\n",
    "# set early stopping parameters\n",
    "patience = 3\n",
    "epochs = 100\n",
    "model = My_nn()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_function = nn.MSELoss()\n",
    "delta = 0.001\n",
    "count = 0\n",
    "min_val_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train(train_set, model, optimizer, loss_function)\n",
    "    val_loss = validate(val_set, model, loss_function)\n",
    "    if val_loss < min_val_loss - delta:\n",
    "        min_val_loss = val_loss\n",
    "        count = 0\n",
    "    else:\n",
    "        count += 1\n",
    "        if count >= patience:\n",
    "            print(f\"Early Stopping at epoch {epoch}\")\n",
    "            break\n",
    "# Save and load best model\n",
    "torch.save(model.state_dict(), 'best_model.pth')\n",
    "best_model = My_nn()\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "best_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4267d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tensorboard \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "# define directory for saving the info \n",
    "log_dir = './logs'\n",
    "writer = SummaryWriter(log_dir) # Writer will output to ./runs/ directory by default\n",
    "# Now we can use writer to \"wrire\" different values in training loop\n",
    "writer.add_scaler('Traning Loss', loss, global_step = step)\n",
    "writer.add_scaler('Training Accuracy', acc, global_step = step)\n",
    "writer.add_image('images', img_grid) #img_grid = torchvision.utils.make_grid(data)\n",
    "writer.add_hparam({'learning rate': lr, 'batch_size': bs}) #if running the model with multiple batch sizes\n",
    "writer.add_histogram('fc1'. model.fc1.weight)\n",
    "# then we close the writer:\n",
    "writer.close()\n",
    "# then run the tensor board in anaconda \n",
    "tensorboard --logdir=./logs  # = runs\n",
    "# Then, navigate to http://localhost:6006 in your web browser to view the TensorBoard dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## batch normalization and layer normalization:\n",
    "# Batch normalization: Normalizing across the batch dimension calculates normalization statistics (mean and variance) \n",
    "# for each feature (or channel) across all samples in the batch\n",
    "# Layer Normalziation: Normalizing across the feature dimension calculates normalization statistics for each sample \n",
    "# individually, across all features\n",
    "# nn.BatchNorm1d(input_size)  # comes after activation\n",
    "# nn.LayerNorm(input_size) # comes after activation \n",
    "# nn.Dropout(dropout) # comes after LN\n",
    "# Define the neural network model\n",
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)  # Batch Normalization\n",
    "        self.ln1 = nn.LayerNorm(512)    # Layer Normalization\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)  # Flatten the input\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.bn1(x)  # Apply Batch Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.ln1(x)  # Apply Layer Normalization\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "791d6db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Tensor:\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "\n",
      "Mask:\n",
      "tensor([[ True, False,  True],\n",
      "        [False,  True, False],\n",
      "        [ True, False,  True]])\n",
      "\n",
      "Result after masked_fill:\n",
      "tensor([[0, 2, 0],\n",
      "        [4, 0, 6],\n",
      "        [0, 8, 0]])\n"
     ]
    }
   ],
   "source": [
    "# masked_fill used in transfromers and autoregressive models\n",
    "# Define the input tensor\n",
    "tensor = torch.tensor([[1, 2, 3],\n",
    "                       [4, 5, 6],\n",
    "                       [7, 8, 9]])\n",
    "\n",
    "# Define the mask tensor\n",
    "mask = torch.tensor([[True, False, True],\n",
    "                     [False, True, False],\n",
    "                     [True, False, True]])\n",
    "\n",
    "# Define the value to fill\n",
    "value = 0\n",
    "\n",
    "# Use masked_fill to replace values based on the mask; masekd_fill_ does masking in place \n",
    "result = tensor.masked_fill(mask, value)\n",
    "\n",
    "print(\"Original Tensor:\")\n",
    "print(tensor)\n",
    "print(\"\\nMask:\")\n",
    "print(mask)\n",
    "print(\"\\nResult after masked_fill:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404a9c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Augmentation --> torchvision.transforms.compose / .randomhorizentalflip, ... --> check resource\n",
    "# Example:\n",
    "# Define the transformations for data augmentation\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.RandomCrop(28, padding=4),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# Load the CIFAR-10 dataset with data augmentation\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,\n",
    "                                         shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e33170",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transfer Learning : \n",
    "#1- feature extraction: remove the last layer which is task specific and use the network for extracting features\n",
    "import torchvision.models as models\n",
    "# load resnet 18\n",
    "model = models.resnet18(pretrained = True)\n",
    "# Remove last layer \n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "#2- Fine Tuning: use the model for a new task ny fine tunning some layers and freezing some other \n",
    "for param in model.parameters():\n",
    "    param.requires_gard = True\n",
    "for name, child in model.named_children():\n",
    "    if name in ['Layer1', 'Layer2']:\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = False\n",
    "#3- modify last layer \n",
    "num_ftrs = model.fc.in_features #assuming.fc is defined\n",
    "model.fc = torch.nn.Linear(num_ftrs, 2) \n",
    "# or\n",
    "# Convert model children iterator to a list\n",
    "children = list(model.children())\n",
    "\n",
    "# Access the last layer\n",
    "last_layer = children[-1]\n",
    "\n",
    "# Get the number of input features of the last layer\n",
    "in_ftrs = last_layer.in_features\n",
    "\n",
    "# Create a new linear layer with the desired number of output features\n",
    "new_last_layer = nn.Linear(in_ftrs, 2)\n",
    "\n",
    "# Replace the last layer in the list of children\n",
    "children[-1] = new_last_layer\n",
    "\n",
    "# Set the model's children to the updated list\n",
    "model.children = nn.ModuleList(children)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7a7eba57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    }
   ],
   "source": [
    "#  tensor operations including slicing, indexing, concatenating, and transposing using PyTorch:\n",
    "x = torch.tensor([[1,2,3], [4,5,6]])\n",
    "x[1,2] = 12\n",
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f4012af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5687,  0.3576, -0.6363],\n",
      "        [-0.1427,  0.7910, -0.5951]])\n",
      "tensor([[[ 1.0000,  2.0000,  3.0000],\n",
      "         [ 4.0000,  5.0000, 12.0000]],\n",
      "\n",
      "        [[ 0.5687,  0.3576, -0.6363],\n",
      "         [-0.1427,  0.7910, -0.5951]]])\n",
      "torch.Size([2, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "#concatenate two tensors\n",
    "y = torch.randn(2,3)\n",
    "print(y)\n",
    "z = torch.stack([x,y])\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5d7ac31d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000,  2.0000,  3.0000, -0.9202],\n",
      "        [ 4.0000,  5.0000, 12.0000,  0.7902]])\n",
      "torch.Size([2, 4])\n"
     ]
    }
   ],
   "source": [
    "t = torch.randn(2,1)\n",
    "z = torch.cat([x,t], dim = 1)\n",
    "print(z)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "9319a80b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(4,3,2)\n",
    "y = torch.randn(4,3,2)\n",
    "z = torch.cat([x,y], dim = 1)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f9f8847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "z = torch.stack([x,y])\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "64a7edc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "tensor([[[1, 4]],\n",
      "\n",
      "        [[2, 5]],\n",
      "\n",
      "        [[3, 6]]])\n",
      "torch.Size([3, 1, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[[1,2,3], [4,5,6]]])\n",
    "print(x)\n",
    "z = x.permute(2,0,1)\n",
    "print(z)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "42da63f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "z= x.transpose(-2,-1)\n",
    "print(z.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "296a56a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "print(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "29f9c89a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0741,  0.3608],\n",
      "         [-1.1742, -1.0755],\n",
      "         [-0.0167,  1.8822]]])\n",
      "tensor([[[-0.0741,  0.3608],\n",
      "         [-1.1742, -1.0755],\n",
      "         [-0.0167,  1.8822],\n",
      "         [-0.0741,  0.3608],\n",
      "         [-1.1742, -1.0755],\n",
      "         [-0.0167,  1.8822]]])\n",
      "torch.Size([1, 6, 2])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1,3,2)\n",
    "z = x.repeat(1,2,1)\n",
    "print(x)\n",
    "print(z)\n",
    "print(z.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82c58b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a feedforward NN that uses the MNIST dataset \n",
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torchvision \n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "987bd4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset \n",
    "train = datasets.MNIST(root = '', train = True, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "test = datasets.MNIST('', train = False, download = True, transform = transforms.Compose([transforms.ToTensor()]))\n",
    "# Define an inteable object\n",
    "train_set = torch.utils.data.DataLoader(train, batch_size = 64, shuffle = True)\n",
    "test_set = torch.utils.data.DataLoader(test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e1f7eb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n",
      "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
      "\n",
      "\n",
      "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          ...,\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([5, 9, 2, 1, 5, 4, 7, 1, 0, 4, 5, 6, 1, 5, 8, 1, 0, 1, 3, 5, 4, 9, 6, 0,\n",
      "        7, 7, 9, 2, 2, 4, 0, 6, 9, 8, 7, 7, 4, 3, 8, 4, 9, 6, 0, 5, 6, 1, 5, 3,\n",
      "        1, 3, 3, 6, 1, 0, 9, 1, 8, 2, 8, 6, 2, 7, 1, 9])]\n",
      "torch.Size([64, 1, 28, 28])\n",
      "torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(len(train))\n",
    "print(len(test))\n",
    "# visiualize the data:\n",
    "for data in train_set:\n",
    "    print(data)\n",
    "    break\n",
    "print(data[0].shape)\n",
    "print(data[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ecceb58e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n",
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a sample image\n",
    "x = data[0][8]\n",
    "y = data[1][8]\n",
    "print(y)\n",
    "print(x.shape)\n",
    "plt.imshow(x.view(28,28))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "22e8b880",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 32)\n",
    "        self.fc2 = nn.Linear(32, 64)\n",
    "        self.fc3 = nn.Linear(64, 10)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "model = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6fb15804",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09eb087c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3694, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.3817, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.4308, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "writer = SummaryWriter()\n",
    "epochs = 3\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.0001)\n",
    "loss_function = nn.NLLLoss() #because labels are numbers and not one hot encoded, also last layer in log_softmax (logits)\n",
    "step = 0\n",
    "losses = []\n",
    "accuracies = []\n",
    "for epoch in range(epochs):\n",
    "    for X, y in train_set:\n",
    "        y_pred = model(X.view(-1,28*28))\n",
    "        loss = loss_function(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         correct = (torch.argmax(y_pred, 1) == y).sum().item()\n",
    "#         training_acc = float(correct)/float(y.shape[0])\n",
    "#         step +=  1\n",
    "#         losses.append(loss.item())\n",
    "#         accuracies.append(training_acc)\n",
    "#         writer.add_scalar('Training Loss', loss.item(), global_step = step)\n",
    "#         writer.add_scalar('Training Accuracy', training_acc, global_step = step)\n",
    "#         writer.add_histogram('fc3', model.fc3.weight)\n",
    "#     writer.add_scalar('Average Loss', sum(losses)/len(losses), epoch)\n",
    "#     writer.add_scalar('Average Accuracy', sum(accuracies)/len(accuracies), epoch)\n",
    "    print(loss)\n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75c94249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "print(y_pred.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c34e6d8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.927\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_set:\n",
    "        output = model(X.view(-1,28*28))\n",
    "        for idx, y_pred in enumerate(output):\n",
    "            if torch.argmax(y_pred) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f'Accuracy : {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c07a2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3)\n",
    "        x = torch.randn(28,28).view(-1,1,28,28)\n",
    "        self.in_ftrs = None\n",
    "        self.convs(x)\n",
    "        self.fc1 = nn.Linear(self.in_ftrs, 256)\n",
    "        self.fc2 = nn.Linear(256, 10)\n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "#         print(x.shape)\n",
    "        if self.in_ftrs == None:\n",
    "            self.in_ftrs = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self.in_ftrs)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "conv_model = CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c95ca90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1241, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.1743, grad_fn=<NllLossBackward0>)\n",
      "tensor(0.0902, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#train the model\n",
    "import torch.optim as optim\n",
    "epochs = 3\n",
    "# step = 0\n",
    "# losses = []\n",
    "# accuracies = []\n",
    "optimizer = optim.Adam(conv_model.parameters(), lr = 0.0001)\n",
    "loss_function = nn.NLLLoss() #because labels are numbers and not one hot encoded, also last layer in log_softmax (logits)\n",
    "for epoch in range(epochs):\n",
    "    for X, y in train_set:\n",
    "        y_pred = conv_model(X.view(-1,1,28,28))\n",
    "        loss = loss_function(y_pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         correct = (torch.argmax(y_pred, 1) == y).sum().item()\n",
    "#         training_acc = float(correct)/float(y.shape[0])\n",
    "#         step +=  1\n",
    "#         losses.append(loss.item())\n",
    "#         accuracies.append(training_acc)\n",
    "#         writer.add_scalar('Training Loss', loss.item(), global_step = step)\n",
    "#         writer.add_scalar('Training Accuracy', training_acc, global_step = step)\n",
    "#         writer.add_histogram('fc2', model.fc2.weight)\n",
    "#     writer.add_scalar('Average Loss', sum(losses)/len(losses), epoch)\n",
    "#     writer.add_scalar('Average Accuracy', sum(accuracies)/len(accuracies), epoch)\n",
    "    print(loss)\n",
    "# writer.flush()\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e4180fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.989\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "total = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for X, y in test_set:\n",
    "        output = conv_model(X.view(-1,1,28,28))\n",
    "        for idx, y_pred in enumerate(output):\n",
    "            if torch.argmax(y_pred) == y[idx]:\n",
    "                correct += 1\n",
    "            total += 1\n",
    "print(f'Accuracy : {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1181a2bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.0000e+00, 3.6955e-06]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# write a function to manullay compute gradient of a linear regression model\n",
    "x = torch.tensor([[6, 9, 8, 0], [1,2,3,4]], dtype= torch.float32)\n",
    "y = torch.tensor([1, 0], dtype= torch.float32)\n",
    "w = torch.randn((1, 4), requires_grad = True)\n",
    "b = torch.zeros((1,2), requires_grad = True)\n",
    "# z = w*x\n",
    "# print(z)\n",
    "# print(z.sum(1))\n",
    "iters = 1000\n",
    "lr = 0.01\n",
    "for i in range(iters):\n",
    "    y_pred = (w*x).sum(1) + b\n",
    "    loss = torch.mean((y_pred-y)**2)\n",
    "    loss.backward()\n",
    "    with torch.no_grad():\n",
    "        w -= lr*w.grad\n",
    "        b -= lr*b.grad\n",
    "    w.grad.zero_()\n",
    "    b.grad.zero_()\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5365769",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save and load a model:\n",
    "torch.save(conv_model.state_dict(), 'best_model.pth')\n",
    "torch.save(optimizer.state_dict(), 'best_opt.pth')\n",
    "best_model = CNN()\n",
    "best_opt = optim.Adam(best_model.parameters(), lr = 0.0001)\n",
    "best_opt.load_state_dict(torch.load('best_opt.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfee4b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to handle imbalanced dataset? \n",
    "#1 = Modify loss function - give more wights to minority class\n",
    "class_weights = torch.tensor([5,1,4])\n",
    "loss_function = nn.NLLLoss(weight = class_weights)\n",
    "#2 upsampling \n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state = 123)\n",
    "# we have X and y as features and labels\n",
    "resampled_x, resampled_y = smote.fit_resample(X,y)\n",
    "#random downsample\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state = 123)\n",
    "resampled_x, resampled_y = rus.fit_resample(X,y)\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "random_sampler = WightedRandomSampler(weights, num_samples) #weights and num_samples predefined\n",
    "resampled_data = DataLoader(data, sampler = random_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daacd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducibilty of results:\n",
    "#1- fix the seed: \n",
    "torch.manual_seed(6) \n",
    "torch.cuda.manual_seed(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d0c136d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensor operations\n",
    "A = torch.ones(3,2)\n",
    "B = torch.ones(3,2)\n",
    "c = A + B\n",
    "D = torch.sub(A,0.5) #or A-0.5\n",
    "E = torch.add(A, 0.5) #or A+0.5\n",
    "F = A - B\n",
    "G = A*B\n",
    "GG = A**B #element-wise power\n",
    "H = torch.mul(A,2)\n",
    "I = torch.div(A,2)\n",
    "J = torch.mm(A,B.transpose(0,1)) \n",
    "K = torch.mm(B,torch.t(A))\n",
    "#reshape\n",
    "L = torch.reshape(A, (1,6))\n",
    "M = A.view(6,1)\n",
    "#combine\n",
    "N = torch.stack([A,B])\n",
    "O = torch.cat([A,B], dim = 0)\n",
    "P = torch.cat([A,B], dim =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "1360c8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 2])\n",
      "torch.Size([6, 2])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(N.shape)\n",
    "print(O.shape)\n",
    "print(P.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1f7c7b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t tensor([[0.0753, 0.0783, 0.2454],\n",
      "        [0.4100, 0.9272, 0.2317]])\n",
      "a tensor([[-1.5539, -0.2381, -0.1986],\n",
      "        [ 1.3529, -0.5624, -1.0071]])\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2,3) #unifrom between 0 and 1\n",
    "print('t', t)\n",
    "a = torch.randn(2,3) #normal dist with mean 0 and std 1\n",
    "print('a', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "688411d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more operations\n",
    "q = torch.empty(2,3) #allocates memory space\n",
    "z = torch.randn_like(w) #random matrix same shape as w; other examples: torch.ones_like torch.zero_like\n",
    "#torch.tensor(data) creates a tensor copy of data if data is in shape of list or tuples or a mix of both  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edfd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data types\n",
    "xx = torch.rand((2,3), dtype = torch.float32) #or .float64, .int32, etc\n",
    "xx.to(torch.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "0d4e1550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.4972, 0.3110],\n",
      "         [0.4918, 0.0226],\n",
      "         [0.6509, 0.8509]],\n",
      "\n",
      "        [[0.4972, 0.3110],\n",
      "         [0.4918, 0.0226],\n",
      "         [0.6509, 0.8509]],\n",
      "\n",
      "        [[0.4972, 0.3110],\n",
      "         [0.4918, 0.0226],\n",
      "         [0.6509, 0.8509]],\n",
      "\n",
      "        [[0.4972, 0.3110],\n",
      "         [0.4918, 0.0226],\n",
      "         [0.6509, 0.8509]]])\n",
      "tensor([[[0.5678, 0.5678],\n",
      "         [0.1038, 0.1038],\n",
      "         [0.2897, 0.2897]],\n",
      "\n",
      "        [[0.5678, 0.5678],\n",
      "         [0.1038, 0.1038],\n",
      "         [0.2897, 0.2897]],\n",
      "\n",
      "        [[0.5678, 0.5678],\n",
      "         [0.1038, 0.1038],\n",
      "         [0.2897, 0.2897]],\n",
      "\n",
      "        [[0.5678, 0.5678],\n",
      "         [0.1038, 0.1038],\n",
      "         [0.2897, 0.2897]]])\n",
      "tensor([[[0.6721, 0.9178],\n",
      "         [0.6721, 0.9178],\n",
      "         [0.6721, 0.9178]],\n",
      "\n",
      "        [[0.6721, 0.9178],\n",
      "         [0.6721, 0.9178],\n",
      "         [0.6721, 0.9178]],\n",
      "\n",
      "        [[0.6721, 0.9178],\n",
      "         [0.6721, 0.9178],\n",
      "         [0.6721, 0.9178]],\n",
      "\n",
      "        [[0.6721, 0.9178],\n",
      "         [0.6721, 0.9178],\n",
      "         [0.6721, 0.9178]]])\n"
     ]
    }
   ],
   "source": [
    "## tensor broadcasting rules:\n",
    "# tensors have identical size or\n",
    "# one of the dimensions is one or\n",
    "# one of the dimensions of one does not exist in the other tensor\n",
    "## Dimensions must match last to first \n",
    "a =     torch.ones(4, 3, 2)\n",
    "\n",
    "b =torch.rand(3, 2)*a # 3rd & 2nd dims identical to a, dim 1 absent\n",
    "print(b)\n",
    "\n",
    "c = a * torch.rand(   3, 1) # 3rd dim = 1, 2nd dim identical to a\n",
    "print(c)\n",
    "\n",
    "d = a * torch.rand(   1, 2) # 3rd dim identical to a, 2nd dim = 1\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d619649f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common functions:\n",
      "tensor([[0.1328, 0.5505, 0.4958, 0.6799],\n",
      "        [0.4487, 0.0192, 0.1758, 0.5010]])\n",
      "tensor([[1., -0., 1., -0.],\n",
      "        [-0., 1., -0., 1.]])\n",
      "tensor([[ 0., -1.,  0., -1.],\n",
      "        [-1.,  0., -1.,  0.]])\n",
      "tensor([[ 0.1328, -0.5000,  0.4958, -0.5000],\n",
      "        [-0.4487,  0.0192, -0.1758,  0.5000]])\n",
      "\n",
      "Sine and arcsine:\n",
      "tensor([0.0000, 0.7854, 1.5708, 2.3562])\n",
      "tensor([0.0000, 0.7071, 1.0000, 0.7071])\n",
      "tensor([0.0000, 0.7854, 1.5708, 0.7854])\n",
      "\n",
      "Bitwise XOR:\n",
      "tensor([3, 2, 1])\n",
      "\n",
      "Broadcasted, element-wise equality comparison:\n",
      "tensor([[ True, False],\n",
      "        [False, False]])\n",
      "\n",
      "Reduction ops:\n",
      "tensor(4.)\n",
      "4.0\n",
      "tensor(2.5000)\n",
      "tensor(1.2910)\n",
      "tensor(24.)\n",
      "tensor([1, 2])\n",
      "\n",
      "Vectors & Matrices:\n",
      "tensor([ 0.,  0., -1.])\n",
      "tensor([[0.4217, 0.9537],\n",
      "        [0.6541, 0.5101]])\n",
      "tensor([[1.2652, 2.8610],\n",
      "        [1.9623, 1.5304]])\n",
      "torch.return_types.svd(\n",
      "U=tensor([[-0.7916, -0.6110],\n",
      "        [-0.6110,  0.7916]]),\n",
      "S=tensor([3.8836, 0.9471]),\n",
      "V=tensor([[-0.5666,  0.8240],\n",
      "        [-0.8240, -0.5666]]))\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "# common functions -- Note: most of these functions with _ do the operation in place; e.g., torch.sin_()\n",
    "a = torch.rand(2, 4) * 2 - 1\n",
    "print('Common functions:')\n",
    "print(torch.abs(a))\n",
    "print(torch.ceil(a))\n",
    "print(torch.floor(a))\n",
    "print(torch.clamp(a, -0.5, 0.5))\n",
    "\n",
    "# trigonometric functions and their inverses\n",
    "angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n",
    "sines = torch.sin(angles)\n",
    "inverses = torch.asin(sines)\n",
    "print('\\nSine and arcsine:')\n",
    "print(angles)\n",
    "print(sines)\n",
    "print(inverses)\n",
    "\n",
    "# bitwise operations\n",
    "print('\\nBitwise XOR:')\n",
    "b = torch.tensor([1, 5, 11])\n",
    "c = torch.tensor([2, 7, 10])\n",
    "print(torch.bitwise_xor(b, c))\n",
    "\n",
    "# comparisons:\n",
    "print('\\nBroadcasted, element-wise equality comparison:')\n",
    "d = torch.tensor([[1., 2.], [3., 4.]])\n",
    "e = torch.ones(1, 2)  # many comparison ops support broadcasting!\n",
    "print(torch.eq(d, e)) # returns a tensor of type bool\n",
    "\n",
    "# reductions:\n",
    "print('\\nReduction ops:')\n",
    "print(torch.max(d))        # returns a single-element tensor\n",
    "print(torch.max(d).item()) # extracts the value from the returned tensor\n",
    "print(torch.mean(d))       # average\n",
    "print(torch.std(d))        # standard deviation\n",
    "print(torch.prod(d))       # product of all numbers\n",
    "print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # filter unique elements\n",
    "\n",
    "# vector and linear algebra operations\n",
    "v1 = torch.tensor([1., 0., 0.])         # x unit vector\n",
    "v2 = torch.tensor([0., 1., 0.])         # y unit vector\n",
    "m1 = torch.rand(2, 2)                   # random matrix\n",
    "m2 = torch.tensor([[3., 0.], [0., 3.]]) # three times identity matrix\n",
    "\n",
    "print('\\nVectors & Matrices:')\n",
    "print(torch.cross(v2, v1)) # negative of z unit vector (v1 x v2 == -v2 x v1) \n",
    "print(m1)\n",
    "m3 = torch.matmul(m1, m2)\n",
    "print(m3)                  # 3 times m1\n",
    "print(torch.svd(m3))       # singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e476e816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 6, 39, 16])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,30,5], [5, 9, 11]])\n",
    "print(a.sum(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b505b534",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(2, 2)\n",
    "b = torch.rand(2, 2)\n",
    "c = torch.zeros(2, 2)\n",
    "old_id = id(c)\n",
    "\n",
    "print(c)\n",
    "d = torch.matmul(a, b, out=c)\n",
    "print(c)                # contents of c have changed\n",
    "\n",
    "assert c is d           # test c & d are same object, not just containing equal values\n",
    "assert id(c) == old_id  # make sure that our new c is the same object as the old one\n",
    "\n",
    "torch.rand(2, 2, out=c) # works for creation too!\n",
    "print(c)                # c has changed again\n",
    "assert id(c) == old_id  # still the same object!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2459e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy tensor\n",
    "a = torch.rand(2, 2, requires_grad=True) # turn on autograd\n",
    "print(a)\n",
    "\n",
    "b = a.clone()\n",
    "print(b)\n",
    "\n",
    "c = a.detach().clone() #.detach --> no gradient\n",
    "print(c)\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34334701",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Squeeze and Unsqueeze \n",
    "# unsqueeze adds a dimension of extent one at given position\n",
    "# example use: need to input a single datapoint x: (6,5) as a batch of size 1 to the model -> x.unsqueeze(0) -> x:(1, 6, 5)\n",
    "# unsqueeze is usefull to enable broadcasting; because it adds a dimension of 1 whereever needed \n",
    "# squeeze removes extent of dimension 1 from a given position: x: (1,20) -> x.squeeze(0) -> x: (20, )\n",
    "# x: (2,3) -> x.squeeze(0): no change because pos 0 is not of dimension 1\n",
    "# .squeeze_ and .unsqueeze_ modify the tensor in place "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b6a1996f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 226, 226])\n",
      "torch.Size([1, 3, 226, 226])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(3, 226, 226)\n",
    "b = a.unsqueeze(0)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "77f6e647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[[0.1760]]]]])\n"
     ]
    }
   ],
   "source": [
    "c = torch.rand(1, 1, 1, 1, 1)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9b742814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "tensor([[0.1949, 0.3038, 0.0990, 0.8200, 0.6339, 0.1725, 0.8114, 0.2419, 0.3840,\n",
      "         0.5465, 0.2182, 0.7251, 0.8358, 0.3082, 0.5330, 0.4126, 0.2973, 0.8230,\n",
      "         0.1259, 0.8779]])\n",
      "torch.Size([20])\n",
      "tensor([0.1949, 0.3038, 0.0990, 0.8200, 0.6339, 0.1725, 0.8114, 0.2419, 0.3840,\n",
      "        0.5465, 0.2182, 0.7251, 0.8358, 0.3082, 0.5330, 0.4126, 0.2973, 0.8230,\n",
      "        0.1259, 0.8779])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand(1, 20)\n",
    "print(a.shape)\n",
    "print(a)\n",
    "\n",
    "b = a.squeeze(0)\n",
    "print(b.shape)\n",
    "print(b)\n",
    "\n",
    "c = torch.rand(2, 2)\n",
    "print(c.shape)\n",
    "\n",
    "d = c.squeeze(0)\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "252d8370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1])\n",
      "tensor([[[0.4553, 0.4553],\n",
      "         [0.9682, 0.9682],\n",
      "         [0.8033, 0.8033]],\n",
      "\n",
      "        [[0.4553, 0.4553],\n",
      "         [0.9682, 0.9682],\n",
      "         [0.8033, 0.8033]],\n",
      "\n",
      "        [[0.4553, 0.4553],\n",
      "         [0.9682, 0.9682],\n",
      "         [0.8033, 0.8033]],\n",
      "\n",
      "        [[0.4553, 0.4553],\n",
      "         [0.9682, 0.9682],\n",
      "         [0.8033, 0.8033]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(4, 3, 2)\n",
    "b = torch.rand(   3)     # trying to multiply a * b will give a runtime error\n",
    "c = b.unsqueeze(1)       # change to a 2-dimensional tensor, adding new dim at the end\n",
    "print(c.shape)\n",
    "print(a * c)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2b6513",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Optimization Methods \n",
    "## Stochastic Gradient Descent (SGD):\n",
    "# Hyperparameters:\n",
    "# lr (learning rate): Determines the step size at each iteration.\n",
    "# momentum: Accelerates SGD in relevant directions and dampens oscillations.\n",
    "# Use Cases:\n",
    "# Classic optimization algorithm widely used in training neural networks.\n",
    "# Explanation:\n",
    "# SGD updates the model parameters by moving in the direction opposite to the gradient of the loss function \n",
    "# with respect to the parameters.\n",
    "optimizer_sgd = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "## Adam (Adaptive Moment Estimation):\n",
    "# Hyperparameters:\n",
    "# lr (learning rate): Determines the step size.\n",
    "# betas: Coefficients for computing running averages of gradient and its square.\n",
    "# eps: Term to improve numerical stability.\n",
    "# Use Cases:\n",
    "# Popular choice for training deep neural networks.\n",
    "# Explanation:\n",
    "# Adam combines the advantages of AdaGrad and RMSProp by using both the first-order moment (mean) and the \n",
    "#second-order moment (variance) of the gradients.\n",
    "optimizer_adam = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "## Adagrad (Adaptive Gradient Algorithm):\n",
    "# Hyperparameters:\n",
    "# lr (learning rate): Determines the step size.\n",
    "# lr_decay: Learning rate decay over each update.\n",
    "# eps: Term to improve numerical stability.\n",
    "# Use Cases:\n",
    "# Suitable for sparse data or problems with differing gradients across parameters.\n",
    "# Explanation:\n",
    "# Adagrad adapts the learning rate for each parameter based on the historical gradients for that parameter.\n",
    "optimizer_adagrad = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, eps=1e-10)\n",
    "\n",
    "## RMSprop (Root Mean Square Propagation):\n",
    "# Hyperparameters:\n",
    "# lr (learning rate): Determines the step size.\n",
    "# alpha: Smoothing constant.\n",
    "# eps: Term to improve numerical stability.\n",
    "# Use Cases:\n",
    "# Addresses Adagrad's diminishing learning rates by using a moving average of squared gradients.\n",
    "# Explanation:\n",
    "# RMSprop adapts the learning rates separately for each parameter based on the average of recent magnitudes of \n",
    "# the gradients for that parameter.\n",
    "optimizer_rmsprop = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.99, eps=1e-08)\n",
    "\n",
    "## AdamW (Adam Weight Decay):\n",
    "# Hyperparameters:\n",
    "# Same as Adam, but includes weight decay (weight_decay).\n",
    "# Use Cases:\n",
    "# Variation of Adam that decouples weight decay regularization from the optimization steps.\n",
    "# Explanation:\n",
    "# AdamW is similar to Adam, but it applies weight decay directly to the parameters rather than to the gradient.\n",
    "optimizer_adamw = optim.AdamW(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n",
    "\n",
    "## LBFGS (Limited-memory BroydenFletcherGoldfarbShanno):\n",
    "# Hyperparameters:\n",
    "# max_iter: Maximum number of iterations.\n",
    "# Use Cases:\n",
    "# Useful for problems with relatively small datasets or low-dimensional parameter spaces.\n",
    "# Explanation:\n",
    "# LBFGS is a quasi-Newton optimization algorithm that approximates the inverse Hessian matrix to update parameters.\n",
    "optimizer_lbfgs = optim.LBFGS(model.parameters(), max_iter=20)\n",
    "\n",
    "## SGD with momentum:\n",
    "# Hyperparameters:\n",
    "# lr (learning rate), momentum.\n",
    "# Use Cases:\n",
    "# Accelerates SGD by accumulating a fraction of past gradients.\n",
    "# Explanation:\n",
    "# SGD with momentum helps in reducing oscillations and converging faster in relevant directions.\n",
    "optimizer_sgd_momentum = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "## SGD with Nesterov momentum:\n",
    "# Hyperparameters:\n",
    "# lr (learning rate), momentum.\n",
    "# Use Cases:\n",
    "# Improvement over regular SGD with momentum, particularly in deep learning.\n",
    "# Explanation:\n",
    "# SGD with Nesterov momentum computes the gradient of the loss function at the \"lookahead\" position, \n",
    "# resulting in improved convergence.\n",
    "optimizer_sgd_nesterov = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f7747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building custom blocks used in transfromers\n",
    "1# Layer Normalization --> this has built-in method nn.LayerNorm(features) but lets build\n",
    "# x --> alpha*(x-mean)/(std+eps) + bias\n",
    "class LayerNormalization(nn.Module):\n",
    "    def __init__(self, features, eps):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.alpha = nn.Parameter(torch.ones(features))\n",
    "        self.bias = nn.Parameter(torch.zeros(features))\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True)\n",
    "        std = x.std(dim = -1, keepdim = True)\n",
    "        return self.alpha*(x-mean)/(std+eps) + self.bias\n",
    "\n",
    "#2 Input Embedding\n",
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, vocab_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)\n",
    "\n",
    "#3 FeedForward two fully connected layers with dropout \n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(self.dropout)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    \n",
    "#4 Positional Encoding: pe[2i] = sin(pos/10000**(2*i/d_model)) pe[2i+1] = cos(pos/10000**(2*i/d_model))\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, seq_ln, d_model, dropout):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(seq_ln, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pos = torch.arange(0, seq_ln, dtype = torch.float).unsqueeze(1) #seq_ln*1\n",
    "        div = torch.exp(torch.arange(0, d_model, 2)).float()*(-math.log(10000)/d_model)\n",
    "        pe[:,0::2] = sin(pos*div)\n",
    "        pe[:, 1::2] = cos(pos*div)\n",
    "        pe.unsqueeze(0) #add batch dim\n",
    "        self.register_buffer('pe', pe) # we register encoding as a buffer so it is a Module state and not a param\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.shape[1],:].detach() # detach from computational graph\n",
    "        return self.dropout(x)\n",
    "    \n",
    "#5 Residual connection\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, features, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.norm = LayerNormalization(features)\n",
    "    def forward(self, x, sublayer):\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "\n",
    "#6 Multi Head Attention Block : there is a built-in method nn.MultiHeadAttention(input_dim, number_heads, dropout)\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        assert d_model%num_heads == 0, 'dimension mismatch'\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model//num_heads\n",
    "        self.w_q = nn.Linear(de_model, d_model, bias = False)\n",
    "        self.w_k = nn.Linear(de_model, d_model, bias = False)\n",
    "        self.w_v = nn.Linear(de_model, d_model, bias = False)\n",
    "        self.w_o = nn.Linear(de_model, d_model, bias = False)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):\n",
    "        d_k = query.shape[-1]\n",
    "        attention_scores = (query @ key.transpose(-2,-1))/math.sqrt(d_k)\n",
    "        if mask is not None:\n",
    "            attention_scores.masked_fill(mask, -1e9)\n",
    "        if dropout is not None:\n",
    "            attention_scores = dropout(attention_scores)\n",
    "        return attention_scores @ value, attention_scores\n",
    "    \n",
    "    def forward(self, q, k, v, mask):\n",
    "        query = self.w_q(q)\n",
    "        key = self.w_k(k)\n",
    "        value = self.w_v(v)\n",
    "        # (batch, seq_ln, d_model) -> (batch, seq_ln, h, d_k) -> (batch, h, seq_ln, dk)\n",
    "        query = query.view(query.shape[0], query.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "        value = value.view(value.shape[0], value.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "        key = key.view(key.shape[0], key.shape[1], self.num_heads, self.d_k).transpose(1,2)\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, self.dropout)\n",
    "        x = x.transpose(1,2).contiguous().view(x.shape[0],-1, d_model)\n",
    "        x = self.w_o(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "36426bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 9],\n",
      "        [1, 2, 9],\n",
      "        [1, 2, 9]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 9]])\n",
    "b = a.repeat(3,1)\n",
    "print(b)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
